{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4237adb8-b3fd-4aa8-8b1e-c66e151cef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4cbd1-3e2a-45c8-aae0-1b8f5037ac3f",
   "metadata": {},
   "source": [
    "### 一、召回模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9a50a5-f269-4c0e-a1ad-a1a87edc878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from six import iteritems\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    \"\"\"\n",
    "    BM25模型\n",
    "\n",
    "    Args:\n",
    "        corpus (:obj:`list`):\n",
    "            检索的语料\n",
    "        k1 (:obj:`float`, optional, defaults to 1.5):\n",
    "            取正值的调优参数，用于文档中的词项频率进行缩放控制\n",
    "        b (:obj:`float`, optional, defaults to 0.75):\n",
    "            0到1之间的参数，决定文档长度的缩放程度，b=1表示基于文档长度对词项权重进行完全的缩放，b=0表示归一化时不考虑文档长度因素\n",
    "        epsilon (:obj:`float`, optional, defaults to 0.25):\n",
    "            idf的下限值\n",
    "        tokenizer (:obj:`object`, optional, defaults to None):\n",
    "            分词器，用于对文档进行分词操作，默认为None，按字颗粒对文档进行分词\n",
    "        is_retain_docs (:obj:`bool`, optional, defaults to False):\n",
    "            是否保持原始文档\n",
    "\n",
    "    Reference:\n",
    "        [1] https://github.com/RaRe-Technologies/gensim/blob/3.8.3/gensim/summarization/bm25.py\n",
    "    \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus,\n",
    "        k1=1.5,\n",
    "        b=0.75,\n",
    "        epsilon=0.25,\n",
    "        tokenizer=None,\n",
    "        is_retain_docs=False\n",
    "    ):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.docs = None\n",
    "        self.corpus_size = 0\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "\n",
    "        if is_retain_docs:\n",
    "            self.docs = copy.deepcopy(corpus)\n",
    "\n",
    "        if tokenizer:\n",
    "            corpus = [self.tokenizer.tokenize(document) for document in corpus]\n",
    "        else:\n",
    "            corpus = [list(document) for document in corpus]\n",
    "\n",
    "        self._initialize(corpus)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        \"\"\"Calculates frequencies of terms in documents and in corpus. Also computes inverse document frequencies.\"\"\"\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:                        \n",
    "            self.corpus_size += 1\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                if word not in nd:\n",
    "                    nd[word] = 0\n",
    "                nd[word] += 1\n",
    "\n",
    "        self.avgdl = float(num_doc) / self.corpus_size\n",
    "\n",
    "        idf_sum = 0\n",
    "        negative_idfs = []\n",
    "        for word, freq in iteritems(nd):\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = float(idf_sum) / len(self.idf)\n",
    "\n",
    "        if self.average_idf < 0:\n",
    "            logger.warning(\n",
    "                'Average inverse document frequency is less than zero. Your corpus of {} documents'\n",
    "                ' is either too small or it does not originate from natural text. BM25 may produce'\n",
    "                ' unintuitive results.'.format(self.corpus_size)\n",
    "            )\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "    def get_score(self, query, index):\n",
    "        score = 0.0\n",
    "        doc_freqs = self.doc_freqs[index]\n",
    "        numerator_constant = self.k1 + 1\n",
    "        denominator_constant = self.k1 * (1 - self.b + self.b * self.doc_len[index] / self.avgdl)\n",
    "        for word in query:\n",
    "            if word in doc_freqs:\n",
    "                df = self.doc_freqs[index][word]\n",
    "                idf = self.idf[word]\n",
    "                score += (idf * df * numerator_constant) / (df + denominator_constant)\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        scores = [self.get_score(query, index) for index in range(self.corpus_size)]\n",
    "        return scores\n",
    "\n",
    "    def recall(self, query, topk=5):\n",
    "        scores = self.get_scores(query)\n",
    "        indexs = np.argsort(scores)[::-1][:topk]\n",
    "\n",
    "        if self.docs is None:\n",
    "            return [[i, scores[i]] for i in indexs]\n",
    "        else:\n",
    "            return [[self.docs[i], scores[i]] for i in indexs]\n",
    "\n",
    "bm25_model = pickle.load(open('../checkpoint/recall/bm25_model.pkl', 'rb'))\n",
    "map_dict = pickle.load(open('../checkpoint/recall/map_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43ac4a-454b-46b8-8c23-8f13b9d543d7",
   "metadata": {},
   "source": [
    "### 二、个数预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb1bcfb-f6dc-4270-95cc-d9b073413c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.tc.bert import Bert as TCBert\n",
    "from ark_nlp.model.tc.bert import BertConfig as TCBertConfig\n",
    "from ark_nlp.model.tc.bert import Dataset as TCDataset\n",
    "from ark_nlp.model.tc.bert import Tokenizer as TCTokenizer\n",
    "from ark_nlp.factory.predictor import TCPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a620c1f-4fb2-417d-b545-a5dd2450415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = 'nghuyong/ernie-1.0'\n",
    "predict_num_model_path = '../checkpoint/predict_num/module.pth'\n",
    "predict_num_cat2id_path = '../checkpoint/predict_num/cat2id.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ed1214-4ea8-4d9b-b3f9-56b9a64163e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_num_tokenizer = TCTokenizer(vocab='nghuyong/ernie-1.0', max_seq_len=100)\n",
    "\n",
    "with open(predict_num_cat2id_path, \"rb\") as f:\n",
    "    predict_num_cat2id = pickle.load(f)\n",
    "\n",
    "predict_num_bert_config = TCBertConfig.from_pretrained(\n",
    "    model_class, \n",
    "    num_labels=len(predict_num_cat2id)\n",
    ")\n",
    "\n",
    "predict_num_dl_module = TCBert(config=predict_num_bert_config)\n",
    "\n",
    "predict_num_dl_module.load_state_dict(torch.load(predict_num_model_path, map_location='cuda:0'))\n",
    "predict_num_dl_module = predict_num_dl_module.eval()\n",
    "\n",
    "tc_predictor_instance = TCPredictor(predict_num_dl_module, predict_num_tokenizer, predict_num_cat2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dfa24-3066-4da1-897b-e5e8b8844b60",
   "metadata": {},
   "source": [
    "### 三、相似预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5653dd56-6d4a-4e5d-80d3-764b02629bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.model.tm.bert import Bert as TMBert\n",
    "from ark_nlp.model.tm.bert import BertConfig as TMBertConfig\n",
    "from ark_nlp.model.tm.bert import Dataset as TMDataset\n",
    "from ark_nlp.model.tm.bert import Task as TMTask\n",
    "from ark_nlp.model.tm.bert import Tokenizer as TMTokenizer\n",
    "from ark_nlp.factory.predictor import TMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d4ba179-7b19-4406-9675-6f1e080b88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = 'nghuyong/ernie-1.0'\n",
    "textsim_model_path = '../checkpoint/textsim/module.pth'\n",
    "textsim_cat2id_path = '../checkpoint/textsim/cat2id.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb4197d-1114-4be5-bbef-898a7b091960",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_textsim_tokenizer = TMTokenizer(vocab='nghuyong/ernie-1.0', max_seq_len=100)\n",
    "\n",
    "with open(textsim_cat2id_path, \"rb\") as f:\n",
    "    predict_textsim_cat2id = pickle.load(f)\n",
    "\n",
    "bert_config = TMBertConfig.from_pretrained(model_class, \n",
    "                                         num_labels=len(predict_textsim_cat2id))\n",
    "\n",
    "predict_textsim_module = TMBert(config=bert_config).to('cuda:0')\n",
    "predict_textsim_module.load_state_dict(torch.load(textsim_model_path))\n",
    "predict_textsim_module = predict_textsim_module.eval()\n",
    "\n",
    "tm_predictor_instance = TMPredictor(predict_textsim_module, predict_textsim_tokenizer, predict_textsim_cat2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88153b4-2e77-4da3-a74f-41915528a1db",
   "metadata": {},
   "source": [
    "### 四、疾病标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b165c581-a0e7-4e78-a860-5db8fe23f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.dataset.base._dataset import BaseDataset\n",
    "\n",
    "\n",
    "class PCTestDataset(BaseDataset):\n",
    "        \n",
    "    def _get_categories(self):\n",
    "        return ''\n",
    "    \n",
    "    def _convert_to_dataset(self, data_df):\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        data_df['text_a'] = data_df['text_a'].apply(lambda x: x.lower().strip())\n",
    "        data_df['text_b'] = data_df['text_b'].apply(lambda x: x.lower().strip())\n",
    "        \n",
    "        feature_names = list(data_df.columns)\n",
    "        for index_, row_ in enumerate(data_df.itertuples()):\n",
    "            dataset.append({feature_name_: getattr(row_, feature_name_) \n",
    "                             for feature_name_ in feature_names})\n",
    "            \n",
    "        return dataset\n",
    "\n",
    "    def _convert_to_transfomer_ids(self, bert_tokenizer):\n",
    "        \n",
    "        features = []\n",
    "        for (index_, row_) in enumerate(self.dataset):\n",
    "            input_ids = bert_tokenizer.sequence_to_ids(row_['text_a'], row_['text_b'])\n",
    "            \n",
    "            input_ids, input_mask, segment_ids = input_ids\n",
    "                        \n",
    "            input_a_length = self._get_input_length(row_['text_a'], bert_tokenizer)\n",
    "            input_b_length = self._get_input_length(row_['text_b'], bert_tokenizer)\n",
    "\n",
    "            feature = {\n",
    "                'input_ids': input_ids, \n",
    "                'attention_mask': input_mask, \n",
    "                'token_type_ids': segment_ids\n",
    "            }\n",
    "\n",
    "            if not self.is_test:\n",
    "                label_ids = self.cat2id[row_['label']]\n",
    "                feature['label_ids'] = label_ids\n",
    "\n",
    "            features.append(feature)\n",
    "        \n",
    "        return features        \n",
    "\n",
    "    def _convert_to_vanilla_ids(self, vanilla_tokenizer):\n",
    "        \n",
    "        features = []\n",
    "        for (index_, row_) in enumerate(self.dataset):\n",
    "\n",
    "            input_a_ids = vanilla_tokenizer.sequence_to_ids(row_['text_a'])\n",
    "            input_b_ids = vanilla_tokenizer.sequence_to_ids(row_['text_b'])   \n",
    "\n",
    "            feature = {\n",
    "                'input_a_ids': input_a_ids,\n",
    "                'input_b_ids': input_b_ids\n",
    "            }\n",
    "\n",
    "            if not self.is_test:\n",
    "                label_ids = self.cat2id[row_['label']]\n",
    "                feature['label_ids'] = label_ids\n",
    "            \n",
    "            features.append(feature)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8260f7de-7689-4a33-aa5d-093220393dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operation_icd_name_batch(query_name):\n",
    "    \n",
    "    predict_num = tc_predictor_instance.predict_one_sample(query_name)[0]\n",
    "            \n",
    "    result = []\n",
    "    search_set = set()\n",
    "    batch_list = []\n",
    "\n",
    "    for _index, _search_word in enumerate(\n",
    "         [_result for _results in bm25_model.recall(query_name, topk=1000) for _result in map_dict[_results[0]]]):\n",
    "        \n",
    "        if _search_word not in search_set:\n",
    "            batch_list.append([query_name, _search_word])\n",
    "            search_set.add(_search_word)\n",
    "            \n",
    "        if _index == 200:\n",
    "            break\n",
    "            \n",
    "    if len(batch_list) == 1:\n",
    "        batch_list = [batch_list]\n",
    "        \n",
    "    batch_df = pd.DataFrame(batch_list, columns=['text_a', 'text_b'])\n",
    "            \n",
    "    batch_dataset = PCTestDataset(batch_df, is_test=True)  \n",
    "    batch_dataset.convert_to_ids(predict_textsim_tokenizer)\n",
    "    batch_predict_ = tm_predictor_instance.predict_batch(batch_dataset, return_proba=True)\n",
    "    \n",
    "    statistics = []\n",
    "    for (query_name, recall_result_), predict_ in zip(batch_list, batch_predict_):\n",
    "        if predict_[0] == \"1\":\n",
    "            statistics.append(predict_[-1])\n",
    "            result.append([recall_result_, predict_[0], predict_[-1]])\n",
    "            \n",
    "    if len(result) == 0:\n",
    "        for (query_name, recall_result_), predict_ in zip(batch_list, batch_predict_):\n",
    "            if predict_[-1] > np.median(statistics):\n",
    "                result.append([recall_result_, predict_[0], 1 - predict_[-1]])\n",
    "    \n",
    "    result = sorted(result, key=lambda x: x[-1], reverse=True)  \n",
    "            \n",
    "    if len(result) == 0:\n",
    "        return ''\n",
    "                \n",
    "    if predict_num == '1':\n",
    "        return result[0][0]\n",
    "    elif predict_num == '2':\n",
    "        if len(result) >= 2:\n",
    "            return result[0][0] + '##' + result[1][0]\n",
    "        else:\n",
    "            return result[0][0]\n",
    "    else:\n",
    "        st_word_ = ''\n",
    "        for index_, word_ in enumerate(result):\n",
    "            if word_[-1] > 0.8:\n",
    "                st_word_ += word_[0]\n",
    "                if index_ != len(result) - 1:\n",
    "                    st_word_ += '##'\n",
    "                    \n",
    "            if index_ > 5:\n",
    "                break\n",
    "                    \n",
    "        if st_word_ == '':\n",
    "            st_word_ = result[0][0]\n",
    "            \n",
    "    if st_word_[-1] == '#':\n",
    "        return st_word_[:-2]\n",
    "    \n",
    "    \n",
    "    return st_word_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06913d5-82b2-4cdd-a3c4-e80622581a1a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、生成提交数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58231f6c-ece0-4e7f-b4c7-1a1d02395734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "test_data_df = pd.read_json('../data/source_datasets/CHIP-CDN/CHIP-CDN_test.json')\n",
    "\n",
    "submit = []\n",
    "for text_ in tqdm(test_data_df['text'].to_list()):\n",
    "    predict_ = get_operation_icd_name_batch(text_)\n",
    "    submit.append({\n",
    "        'text': text_,\n",
    "        'normalized_result': predict_\n",
    "    }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8afac52-dbd1-44f8-ae8f-f88bb5c869b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../data/output_datasets/CHIP-CDN_test.json'\n",
    "\n",
    "with open(output_path,'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(submit, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

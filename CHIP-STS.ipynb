{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.model.tm.bert import Bert\n",
    "from ark_nlp.model.tm.bert import BertConfig\n",
    "from ark_nlp.dataset import TMDataset\n",
    "from ark_nlp.model.tm.bert import Task\n",
    "from ark_nlp.model.tm.bert import get_default_model_optimizer\n",
    "from ark_nlp.model.tm.bert import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondTMDataset(TMDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path, \n",
    "        categories=None, \n",
    "        is_retain_dataset=False\n",
    "    ):\n",
    "        super(CondTMDataset, self).__init__(data_path, categories, is_retain_dataset)\n",
    "        \n",
    "        self.conditions = sorted(list(set([data['condition'] for data in self.dataset])))\n",
    "        self.condition2id = dict(zip(self.conditions, range(len(self.conditions))))\n",
    "        \n",
    "    \n",
    "    def _convert_to_transfomer_ids(self, bert_tokenizer):\n",
    "        \n",
    "        features = []\n",
    "        for (index_, row_) in enumerate(self.dataset):\n",
    "            input_ids = bert_tokenizer.sequence_to_ids(row_['text_a'], row_['text_b'])\n",
    "            \n",
    "            input_ids, input_mask, segment_ids = input_ids\n",
    "            \n",
    "            label_ids = self.cat2id[row_['label']]\n",
    "            \n",
    "            input_a_length = self._get_input_length(row_['text_a'], bert_tokenizer)\n",
    "            input_b_length = self._get_input_length(row_['text_b'], bert_tokenizer)\n",
    "            \n",
    "            features.append({\n",
    "                'input_ids': input_ids, \n",
    "                'attention_mask': input_mask, \n",
    "                'token_type_ids': segment_ids, \n",
    "                'condition_ids': self.condition2id[row_['condition']], \n",
    "                'label_ids': label_ids\n",
    "            })\n",
    "        \n",
    "        return features        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_json('../data/source_datasets/CHIP-STS/CHIP-STS_train.json')\n",
    "train_data_df = (train_data_df\n",
    "                 .rename(columns={'text1': 'text_a', 'text2': 'text_b', 'category': 'condition'})\n",
    "                 .loc[:,['text_a', 'text_b', 'condition', 'label']])\n",
    "\n",
    "dev_data_df = pd.read_json('../data/source_datasets/CHIP-STS/CHIP-STS_dev.json')\n",
    "dev_data_df = dev_data_df[dev_data_df['label'] != \"NA\"]\n",
    "dev_data_df = (dev_data_df\n",
    "                 .rename(columns={'text1': 'text_a', 'text2': 'text_b', 'category': 'condition'})\n",
    "                 .loc[:,['text_a', 'text_b', 'condition', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_train_dataset = CondTMDataset(train_data_df)\n",
    "tm_dev_dataset = CondTMDataset(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='nghuyong/ernie-1.0', max_seq_len=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_train_dataset.convert_to_ids(tokenizer)\n",
    "tm_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('nghuyong/ernie-1.0',\n",
    "                                    num_labels=len(tm_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_conditions = len(tm_train_dataset.condition2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.nn.layer.layer_norm_block import CondLayerNormLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondBert(Bert):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        encoder_trained=True,\n",
    "        pooling='cls_with_pooler'\n",
    "    ):\n",
    "        super(CondBert, self).__init__(config, encoder_trained, pooling)\n",
    "        \n",
    "        self.condition_embed = nn.Embedding(config.num_conditions, config.hidden_size)\n",
    "        nn.init.uniform_(self.condition_embed.weight.data)\n",
    "        \n",
    "        self.cond_layer_normal = CondLayerNormLayer(config.hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        condition_ids=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "                \n",
    "        condition_feature = self.condition_embed(condition_ids)\n",
    "        \n",
    "        encoder_feature = self.cond_layer_normal(outputs.hidden_states[-1], condition_feature)\n",
    "\n",
    "        encoder_feature = self.mask_pooling(encoder_feature, attention_mask)\n",
    "\n",
    "        encoder_feature = self.dropout(encoder_feature)\n",
    "        out = self.classifier(encoder_feature)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = CondBert.from_pretrained('nghuyong/ernie-1.0',\n",
    "                                     config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'ce', cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tm_train_dataset, \n",
    "          tm_dev_dataset,\n",
    "          lr=2e-5,\n",
    "          epochs=num_epoches, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、生成提交数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from ark_nlp.factory.predictor.text_match import TMPredictor\n",
    "\n",
    "class CondTMPredictor(TMPredictor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        module,\n",
    "        tokernizer,\n",
    "        cat2id,\n",
    "        condition2id\n",
    "    ):\n",
    "        super(CondTMPredictor, self).__init__(module, tokernizer, cat2id)\n",
    "        self.condition2id = condition2id\n",
    "\n",
    "    def _convert_to_transfomer_ids(\n",
    "        self,\n",
    "        text_a,\n",
    "        text_b,\n",
    "        condition\n",
    "    ):\n",
    "        input_ids = self.tokenizer.sequence_to_ids(text_a, text_b)\n",
    "        input_ids, input_mask, segment_ids = input_ids\n",
    "\n",
    "        features = {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': input_mask,\n",
    "                'token_type_ids': segment_ids,\n",
    "                'condition_ids': np.array([self.condition2id[condition]])\n",
    "            }\n",
    "        return features\n",
    "\n",
    "    def _get_input_ids(\n",
    "        self,\n",
    "        text_a,\n",
    "        text_b,\n",
    "        condition\n",
    "    ):\n",
    "        if self.tokenizer.tokenizer_type == 'transfomer':\n",
    "            return self._convert_to_transfomer_ids(text_a, text_b, condition)\n",
    "        else:\n",
    "            raise ValueError(\"The tokenizer type does not exist\")\n",
    "\n",
    "    def predict_one_sample(\n",
    "        self,\n",
    "        text,\n",
    "        condition,\n",
    "        topk=None,\n",
    "        return_label_name=True,\n",
    "        return_proba=False\n",
    "    ):\n",
    "        if topk is None:\n",
    "            topk = len(self.cat2id) if len(self.cat2id) > 2 else 1\n",
    "        text_a, text_b = text\n",
    "        features = self._get_input_ids(text_a, text_b, condition)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            logit = self.module(**inputs)\n",
    "            logit = torch.nn.functional.softmax(logit, dim=1)\n",
    "\n",
    "        probs, indices = logit.topk(topk, dim=1, sorted=True)\n",
    "\n",
    "        preds = []\n",
    "        probas = []\n",
    "        for pred_, proba_ in zip(indices.cpu().numpy()[0], probs.cpu().numpy()[0].tolist()):\n",
    "\n",
    "            if return_label_name:\n",
    "                pred_ = self.id2cat[pred_]\n",
    "\n",
    "            preds.append(pred_)\n",
    "\n",
    "            if return_proba:\n",
    "                probas.append(proba_)\n",
    "\n",
    "        if return_proba:\n",
    "            return list(zip(preds, probas))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def predict_batch(\n",
    "        self,\n",
    "        test_data,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        return_label_name=True,\n",
    "        return_proba=False\n",
    "    ):\n",
    "        self.inputs_cols = test_data.dataset_cols\n",
    "\n",
    "        preds = []\n",
    "        probas = []\n",
    "\n",
    "        self.module.eval()\n",
    "        generator = DataLoader(test_data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, inputs in enumerate(generator):\n",
    "                inputs = self._get_module_batch_inputs(inputs)\n",
    "\n",
    "                logits = self.module(**inputs)\n",
    "\n",
    "                preds.extend(torch.max(logits, 1)[1].cpu().numpy())\n",
    "                if return_proba:\n",
    "                    logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "                    probas.extend(logits.max(dim=1).values.cpu().detach().numpy())\n",
    "\n",
    "        if return_label_name:\n",
    "            preds = [self.id2cat[pred_] for pred_ in preds]\n",
    "\n",
    "        if return_proba:\n",
    "            return list(zip(preds, probas))\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_predictor_instance = CondTMPredictor(model.module, tokenizer, tm_train_dataset.cat2id, tm_train_dataset.condition2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_json('../data/source_datasets/CHIP-STS/CHIP-STS_test.json')\n",
    "\n",
    "submit = []\n",
    "for _id, _text_a, _text_b, _condition in zip(test_df['id'], test_df['text1'], test_df['text2'], test_df['category']):\n",
    "    if _condition == 'daibetes':\n",
    "        _condition = 'diabetes'\n",
    "\n",
    "    predict_ = tm_predictor_instance.predict_one_sample([_text_a, _text_b], _condition)[0] \n",
    "    \n",
    "    submit.append({\n",
    "        'id': str(_id),\n",
    "        'text1': _text_a,\n",
    "        'text2': _text_b,\n",
    "        'label': predict_,\n",
    "        'category': _condition\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_path = '../data/output_datasets/CHIP-STS_test.json'\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(submit, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
